{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import uproot as up\n",
    "from multiprocessing import Pool\n",
    "import h5py as h5\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntuple_cache_path = '/data/massive1/LLP/hi_met/cache_ntuple_2024.h5'\n",
    "nano_cache_path = '/data/massive1/LLP/hi_met/cache_nano_2024.h5'\n",
    "\n",
    "ntuple_files = Path('/data/massive1/LLP/store/group').rglob('*.root')\n",
    "nano_files = Path('/data/massive1/LLP/store/data').rglob('*.root')\n",
    "\n",
    "ntuple_k_map = {str(p.with_suffix('').relative_to(p.parents[5]).__str__().replace('/', '=')):p for p in ntuple_files}\n",
    "nano_k_map = {str(p.with_suffix('').relative_to(p.parents[5]).__str__().replace('/', '=')):p for p in nano_files}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_key(k: str, fname: str|Path) -> np.ndarray:\n",
    "    with h5.File(fname, 'r') as f:\n",
    "        arr = f[k][()]\n",
    "        return arr\n",
    "\n",
    "def worker(x:tuple[str, str|Path]):\n",
    "    key, fname = x\n",
    "    return key, load_key(key, fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5.File(ntuple_cache_path, 'r') as f:\n",
    "    ntuple_keys = list(f.keys())\n",
    "\n",
    "with h5.File(nano_cache_path, 'r') as f:\n",
    "    nano_keys = list(f.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def era_from_path(path:Path) -> str:\n",
    "    if path.parents[5].name == 'NANOAOD':\n",
    "        return path.parents[7].name[3:]\n",
    "    else:\n",
    "        return path.parents[2].name.split('_')[4][3:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = Pool(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading NTuples: 100%|██████████| 2582/2582 [00:08<00:00, 296.09it/s]\n"
     ]
    }
   ],
   "source": [
    "r = pool.imap(worker, [(k, ntuple_cache_path) for k in ntuple_keys])\n",
    "ntuple = {}\n",
    "for k,arr in tqdm(r, total=len(ntuple_keys), desc='Loading NTuples'):\n",
    "    path = ntuple_k_map[k]\n",
    "    era = era_from_path(path)\n",
    "    ntuple.setdefault(era, {})[k] = set(tuple(x) for x in arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = h5.File(nano_cache_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f['NANOAOD_22Sep2023-v1_2550000_09bd0d81-c724-4762-abbd-3ebe89f0bbca']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4657"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nano_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nano_k_map2 = {k.split('=')[-1]:v for k,v in nano_k_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Nano: 100%|██████████| 4657/4657 [00:48<00:00, 95.34it/s] \n"
     ]
    }
   ],
   "source": [
    "r = pool.imap(worker, [(k, nano_cache_path) for k in nano_keys])\n",
    "nano = {}\n",
    "for k,arr in tqdm(r, total=len(nano_keys), desc='Loading Nano'):\n",
    "    path = nano_k_map[k.split('_')[-1]]\n",
    "    era = era_from_path(path)\n",
    "    nano.setdefault(era, {})[k] = arr\n",
    "# nano = dict(tqdm(r, total=len(nano_keys), desc='Loading Nano'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_events = {k:[sum(len(vv) for vv in v.values()),0] for k,v in ntuple.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 3883/4657 [20:27<04:04,  3.16it/s]\n"
     ]
    }
   ],
   "source": [
    "matchs = {}\n",
    "with tqdm(total=sum(len(x) for x in nano.values())) as pbar:\n",
    "    for era, d_nano in nano.items():\n",
    "        if era not in ntuple:\n",
    "            continue\n",
    "        d_ntuple = ntuple[era]\n",
    "        matchs[era] = {}\n",
    "        for p_nano, arr_nano in d_nano.items():\n",
    "            nano_set = set(tuple(x) for x in arr_nano)\n",
    "            for p_ntuple, arr_ntuple in d_ntuple.items():\n",
    "                ntuple_set = arr_ntuple\n",
    "                n_match = len(ntuple_set & nano_set)\n",
    "                if n_match > 0:\n",
    "                    matchs[era].setdefault(p_nano, []).append(p_ntuple)\n",
    "                    n_events[era][1] += n_match\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2024A': [0, 0],\n",
       " '2024B': [87633, 87633],\n",
       " '2024C': [2923349, 2923349],\n",
       " '2024D': [3366570, 3366570],\n",
       " '2024E': [5559226, 5559226],\n",
       " '2024F': [5700779, 5700779]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/massive1/LLP/hi_met/matchs2.json', 'w') as f:\n",
    "    json.dump(matchs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/massive1/LLP/hi_met/matchs2.json', 'r') as f:\n",
    "    matchs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_matchs = {}\n",
    "for era, d in matchs.items():\n",
    "    path_matchs[era] = {}\n",
    "    for nano_name, ntuple_names in d.items():\n",
    "        ntuple_paths = [ntuple_k_map[n] for n in ntuple_names]\n",
    "        nano_path = nano_k_map[nano_name]\n",
    "        path_matchs[era][nano_path] = ntuple_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_path = Path('/tmp/merge_ntuples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path = Path('/data/massive1/LLP/cache_2024')\n",
    "out_base = Path('/data/massive1/analysis_data/LLP/hi_met/ntuple_merged_2024')\n",
    "args = []\n",
    "for era in path_matchs:\n",
    "    out_dir = out_base / era\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    (tmp_path / 'lists'/ era).mkdir(parents=True, exist_ok=True)\n",
    "    for nano_path, ntuple_paths in path_matchs[era].items():\n",
    "        out_path = out_dir / nano_path.name\n",
    "        inp_nano_path = tmp_path / 'lists'/ era / f'{nano_path.stem}_nano.txt'\n",
    "        inp_ntuple_path = tmp_path / 'lists' / era / f'{nano_path.stem}_ntuple.txt'\n",
    "        inp_nano_path.write_text(str(nano_path))\n",
    "        inp_ntuple_path.write_text('\\n'.join(str(x) for x in ntuple_paths))\n",
    "        \n",
    "        arg = f'{inp_ntuple_path} {inp_nano_path} {out_path} {cache_path}'\n",
    "        args.append(arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_file = Path('/tmp/inp.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "722537"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_file.write_text('\\n'.join(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
